"""Processing and analysis of fluorescence spectra."""

from __future__ import annotations

import warnings
import numpy as np
from numba import njit, jit, prange
from jacobi import propagate
from matplotlib import gridspec
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import h5py

from .util import parse_roi, parse_calib, parse_qeff

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from typing import Literal
    from matplotlib.axes import Axes
    from matplotlib.figure import Figure
    from numpy.typing import ArrayLike, NDArray
    from agepy.spec.photons.anodes import PositionAnode


class Spectrum:
    """Fluorescence spectrum recorded with a position-sensitive
    detector stored as event (photon hit) coordinates x, y.

    In most cases it is recommended to use the `from_h5` class method
    to load the data from an h5 file generated by metro2hdf.

    Parameters
    ----------
    xy: np.ndarray, shape (N,2)
        Array containing x and y values of photon hits.
    time: int, optional
        Measurement time in seconds for normalization.
    **norm: np.ndarray, shape (2,)
        Additional normalization parameters as keyword arguments
        like the upstream intensity or target density. The values
        can be either floats or an array of two floats (val, err).

    """

    def __init__(
        self,
        xy: NDArray,
        time: int | None = None,
        **norm: NDArray,
    ) -> None:
        # Store the passed data
        self._xy = xy
        self._t = time
        self._norm = list(norm.keys())
        for key, value in norm.items():
            setattr(self, key, value)

    @classmethod
    def from_h5(
        cls,
        file_path: str,
        anode: PositionAnode,
        raw: str = "dld_rd#raw",
        step: str = "0.0",
        time: int | None = None,
        **normalize: str,
    ) -> Spectrum:
        """Load a Spectrum from an h5 file generated by metro2hdf.

        Parameters
        ----------
        file_path: str
            Path to the h5 file.
        anode: PositionAnode
            Anode object to process the raw data.
        raw: str, optional
            Path to the raw data in the h5 file.
        step: str, optional
            Step at which the data is stored. If the measurement
            was not a scan, the step is `"0.0"`.
        time: int, optional
            Measurement time in seconds for normalization.
        **normalize: str
            Path to additional normalization parameters as keyword
            arguments like the upstream intensity or target density.

        Returns
        -------
        spec: Spectrum
            Instance containing the loaded and processed events.

        """
        with h5py.File(file_path, "r") as h5:
            # Metro always appends a /0 to the path
            group_raw = raw + "/0"

            # Check if the data is found
            if group_raw not in h5:
                errmsg = f"{group_raw} not found."
                raise ValueError(errmsg)

            if step not in h5[group_raw]:
                errmsg = f"{step} not found in {group_raw}"
                raise ValueError(errmsg)

            # Load the raw data
            group_raw = h5[group_raw]
            data_raw = np.asarray(group_raw[step])

            # Load normalization values
            for name_norm, group_norm in normalize.items():
                group_norm = group_norm + "/0"

                if group_norm not in h5:
                    errmsg = f"{group_norm} not found."
                    raise ValueError(errmsg)

                # Load the dataset / group
                data_norm = h5[group_norm]

                # Handle different types of recorded data
                if isinstance(data_norm, h5py.Dataset):
                    # Values averaged by metro
                    data_norm = np.asarray(data_norm)

                    if data_norm.shape == (1,):
                        normalize[name_norm] = np.array([data_norm[0], 0])

                    elif data_norm.ndim == 1:
                        idx = list(group_raw.keys()).index(step)
                        normalize[name_norm] = np.array([data_norm[idx], 0])

                    else:
                        errmsg = f"Could not parse data in {group_norm}."
                        raise ValueError(errmsg)

                elif step in data_norm:
                    # Values recorded by metro every x seconds
                    data_norm = np.asarray(data_norm[step])
                    mean = np.mean(data_norm)
                    std = np.std(data_norm, ddof=1, mean=mean)
                    normalize[name_norm] = np.array([mean, std]).flatten()

                else:
                    errmsg = f"{step} not found in {group_norm}."
                    raise ValueError(errmsg)

        # Initialize the Spectrum
        return cls(anode.process(data_raw), time=time, **normalize)

    def xy(self) -> tuple[NDArray, NDArray]:
        """Get the event coordinates (x, y).

        Returns
        -------
        x: np.ndarray, shape (N,)
            The x values of the photon hits.
        y: np.ndarray, shape (N,)
            The y values of the photon hits.

        """
        return self._xy[:, 0], self._xy[:, 1]

    def det_image(
        self,
        bins: int | ArrayLike = 512,
        x_lim: tuple[float, float] = (0, 1),
        y_lim: tuple[float, float] = (0, 1),
        figsize: tuple[float, float] = (6.4, 6.4),
        num: str | int | None = None,
        fig: Figure | None = None,
        ax: tuple[Axes, Axes, Axes, Axes] | None = None,
    ) -> tuple[Figure, tuple[Axes, Axes, Axes, Axes]]:
        """Plot the detector image of the spectrum.

        Parameters
        ----------
        bins: int or array_like or [int, int] or [array, array], optional
            See `numpy.histogram2d`.
        x_lim: [float, float], optional
            Set plot limits on the image x axis and x projection.
        y_lim: [float, float], optional
            Set plot limits on the image y axis and y projection.
        figsize: [float, float], optional
            Figure size in inches.
        num: str or int, optional
            See `matplotlib.pyplot.figure`.
        figure: Figure, optional
            Pass a figure and axes to plot on. If `None` a new
            figure and axes are created.
        ax: [Axes, Axes, Axes, Axes], optional
            Pass a figure and axes to plot on. If `None` a new
            figure and axes are created.

        Returns
        -------
        fig: Figure
            See `matplotlib.figure.Figure`.
        ax: [Axes, Axes, Axes, Axes]
            See `matplotlib.axes.Axes`.

        """
        # Create the figure
        if fig is None or ax is None:
            fig = plt.figure(num=num, figsize=figsize, clear=True)

            # grid with columns=2, row=2
            gs = gridspec.GridSpec(
                2,
                2,
                width_ratios=[3, 1],
                height_ratios=[1, 3],
                wspace=0.05,
                hspace=0.05,
            )

            # 2d detector image is subplot 2: lower left
            ax_det = plt.subplot(gs[2])

            # x projection is subplot 0: upper left
            ax_x = plt.subplot(gs[0], sharex=ax_det)

            # y projection is subplot 3: lower right
            ax_y = plt.subplot(gs[3], sharey=ax_det)

            # colorbar is subplot 1: upper right
            ax_cb = plt.subplot(gs[1])
            ax_cb.axis("off")
            ax_cb_inset = ax_cb.inset_axes([0.0, 0.0, 0.25, 1.0])

            # Remove x and y tick labels
            ax_x.tick_params(axis="both", labelbottom=False)
            ax_y.tick_params(axis="both", labelleft=False)

            # Remove grid from the detector image and colorbar
            ax_det.grid(False)
            ax_cb_inset.grid(False)

        else:
            # Use given axes
            ax_det, ax_x, ax_y, ax_cb_inset = ax

            # Clear the given axes before plotting
            ax_det.clear()
            ax_x.clear()
            ax_y.clear()
            ax_cb_inset.clear()

        # Get the data
        x, y = self.xy()

        # Histogram the data
        hist_xy, x_edges, y_edges = np.histogram2d(
            x, y, bins=bins, range=(0, 1)
        )

        # Define a meshgrid
        x_mesh, y_mesh = np.meshgrid(x_edges, y_edges)

        # Get a colormap from matplotlib
        cmap = plt.get_cmap("YlOrBr_r")

        # Get color for the projections
        color = cmap(0)

        # Set the lowest value to white
        colors = cmap(np.linspace(0, 1, cmap.N))
        colors[0] = (1, 1, 1, 1)

        # Create a colormap with white as the lowest value
        cmap = mcolors.ListedColormap(colors)

        # Plot the detector image
        pcm = ax_det.pcolormesh(
            x_mesh, y_mesh, hist_xy.T, cmap=cmap, rasterized=True
        )

        # Create a colorbar
        fig.colorbar(pcm, cax=ax_cb_inset)

        # Project the detector image onto the x and y axes
        hist_x = np.histogram(x, bins=x_edges)[0]
        hist_y = np.histogram(y, bins=y_edges)[0]

        # Plot the x and y projections
        ax_x.stairs(hist_x, x_edges, color=color)
        ax_y.stairs(hist_y, y_edges, color=color, orientation="horizontal")

        # Remove the first tick label of the x and y projection
        plt.setp(ax_x.get_yticklabels()[0], visible=False)
        plt.setp(ax_y.get_xticklabels()[0], visible=False)

        # Set the limits (this changes the positon of ax_det)
        ax_det.set_xlim(x_lim)
        ax_x.set_xlim(x_lim)
        ax_det.set_ylim(y_lim)
        ax_y.set_ylim(y_lim)

        # Set the labels
        ax_det.set_xlabel("x [arb. u.]")
        ax_det.set_ylabel("y [arb. u.]")

        return fig, (ax_det, ax_x, ax_y, ax_cb_inset)

    def counts(
        self,
        roi: ArrayLike = ((0, 1), (0, 1)),
        bkg: Spectrum | float | int = 0,
    ) -> tuple[float, float]:
        """Get the number of counts in the spectrum and the estimated
        uncertainty.

        Parameters
        ----------
        roi: array_like, shape (2,2), optional
            Region of interest for the detector in the form
            `((xmin, xmax), (ymin, ymax))`. If not provided, the
            full detector is used.
        bkg: Spectrum or float, optional
            Background spectrum (dark counts) to be subtracted that can
            be provided either as an instance of `Spectrum` or as a
            float. For this to work properly, the both spectra
            should be normalized to their measurement duration.

        Returns
        -------
        val: float
            The number of counts (normalized to measurement duration
            and any normalization parameters if available).
        err: float
            The propagated Poisson uncertainty.

        """
        # Coordinates of photon hits (x, y)
        data = np.copy(self._xy)

        # Parse given roi
        roi = np.asarray(roi)

        if roi.shape != (2, 2):
            errmsg = "roi must be shape (2,2)"
            raise ValueError(errmsg)

        # Apply y roi filter
        data = data[data[:, 1] > roi[1][0]]
        data = data[data[:, 1] < roi[1][1]]

        # Discard y values
        data = data[:, 0].flatten()

        # Apply x roi filter
        data = data[data > roi[0][0]]
        data = data[data < roi[0][1]]

        # Calculate the number of counts and the Poisson uncertainty
        val = len(data)
        err = np.sqrt(val)

        # Normalize data to measurement duration
        if self._t is not None:
            val /= self._t
            err /= self._t

        # Subtract background before further normalization
        if isinstance(bkg, Spectrum):
            bkg_val, bkg_err = bkg.counts(roi=roi, bkg=None)
            # Using just the statistical uncertainty of the background
            # counts would underestimate the uncertainty of the subtraction
            bkg_err = np.sqrt(bkg_val * self._t) / self._t
            val = max(val - bkg_val, 0)
            err = np.sqrt(err**2 + bkg_err**2)

        elif isinstance(bkg, int) or isinstance(bkg, float):
            val = max(val - bkg, 0)

        else:
            errmsg = "bkg must be Spectrum, int or float"
            raise TypeError(errmsg)

        # Normalize data to account for beam intensity, gas
        # pressure, etc.
        for normalize in self._norm:
            norm_val, norm_err = getattr(self, normalize)
            err = np.sqrt(
                err**2 / norm_val**2 + norm_err**2 * val**2 / norm_val**4
            )
            val /= norm_val

        # Return the counts and the uncertainty
        return val, err

    def spectrum(
        self,
        bins: int | ArrayLike,
        roi: ArrayLike = ((0, 1), (0, 1)),
        qeff: tuple[NDArray, NDArray, NDArray] | None = None,
        bkg: Spectrum | None = None,
        calib: ArrayLike = ((0, 0), (1, 0)),
        uncertainties: Literal["montecarlo", "gauss"] = "montecarlo",
        mc_samples: int = 10000,
    ) -> tuple[NDArray, NDArray]:
        """Calculate the spectum and its uncertainties for a given
        set of bin edges.

        Parameters
        ----------
        bins: int or array_like
            Bin edges for the histogram. For a calibrated spectrum,
            these should be in wavelength units. For an uncalibrated
            spectrum, these should be between 0 and 1.
        roi: array_like, shape (2,2), optional
            Region of interest for the detector in the form
            `((xmin, xmax), (ymin, ymax))`. If not provided, the
            full detector is used.
        qeff: [np.ndarray, np.ndarray, np.ndarray], optional
            Detector efficiencies in the form `(values, errors, x)`.
            The efficiencies are interpolated to 512 points between
            0 and 1.
        bkg: Spectrum, optional
            Background spectrum (dark counts) to be subtracted.
            For this to work properly, both spectra must be normalized
            to their measurement duration.
        calib: array_like, shape (2,2), optional
            Wavelength calibration parameters in the form
            `((a0, err), (a1, err))`, where `a0` and `a1`
            correspond to $\\lambda = a_1 x + a_0$ and `err` to the
            respective uncertainties.
        uncertainties: Literal["montecarlo", "gauss"], optional
            Error propagation method for handling the uncertainties of
            the efficiencies and the wavelength calibration. If
            `qeff = None` and `calib = None`, this setting has no
            effect. Can be 'montecarlo', or 'none'.
        mc_samples: int, optional
            Number of Monte Carlo samples to use for error propagation.
            Has no effect if `err_prop = 'none'`.

        Returns
        -------
        spec: np.ndarray, shape (N-1,)
            The spectrum in the form of bin values.
        err: np.ndarray, shape (N-1,)
            The respective uncertainties of the bin values.

        """
        # Get x and y values of the photon hits
        data = np.copy(self._xy)

        # Parse wavelength calibration parameters
        calib = parse_calib(calib)

        # Parse the xedges
        xrange = (calib[0, 0], calib[0, 0] + calib[1, 0])
        xrange = (min(xrange), max(xrange))
        xedges = np.histogram([], bins=bins, range=xrange)[1]
        nbins = xedges.shape[0] - 1

        # Parse the region of interest
        roi = parse_roi(roi)

        # Apply y roi filter
        data = data[data[:, 1] > roi[1, 0]]
        data = data[data[:, 1] < roi[1, 1]]

        # Don't need y values anymore
        data = data[:, 0].flatten()

        if data.shape == (0,):
            wrnmsg = "Empty region of interest."
            warnings.warn(wrnmsg, stacklevel=1)
            return np.zeros(nbins), np.zeros(nbins)

        # Prepare the background subtraction
        if bkg is not None:
            # Test if background and data both have a time value
            if bkg._t is None or self._t is None:
                errmsg = "Both background and data must have a time value."
                raise ValueError(errmsg)

            # Get the x and y values of the background spectrum
            bkg_data = np.copy(bkg._xy)

            # Apply y roi filter
            bkg_data = bkg_data[bkg_data[:, 1] > roi[1, 0]]
            bkg_data = bkg_data[bkg_data[:, 1] < roi[1, 1]]

            # Don't need y values anymore
            bkg_data = bkg_data[:, 0].flatten()

            bkg_ratio = self._t / bkg._t

        # Adjust x roi filter to wavelength binning
        wl_min = calib[1, 0] * roi[0, 0] + calib[0, 0]
        wl_max = calib[1, 0] * roi[0, 1] + calib[0, 0]
        roi = np.argwhere((xedges < wl_min) | (xedges > wl_max)).flatten()

        # Parse the quantum efficiencies
        if qeff is not None:
            qeff = parse_qeff(*qeff)

        if uncertainties == "montecarlo":
            # Initialize the random number generator
            rng = np.random.default_rng()

            # Initialize array for storing the sample results
            spectrum = np.zeros((mc_samples, nbins), dtype=np.float64)

            spectrum = montecarlo_spectrum_nobkg(
                spectrum,
                data,
                xedges,
                rng,
                mc_samples,
                calib,
                qeff,
            )

            # Calculate mean and standard deviation of the sampled spectra
            # spectrum = np.mean(mc_spectrum, axis=0)
            errors = np.std(spectrum, ddof=1, axis=0)
            spectrum = np.mean(spectrum, axis=0)

        elif uncertainties == "gauss":
            # Calibrate the data
            data = calib[1, 0] * data + calib[0, 0]

            # Histogram the data without the efficiencies
            spectrum = np.histogram(data, bins=xedges)[0]
            spectrum = np.asarray(spectrum, dtype=np.float64)
            errors = np.sqrt(spectrum)

            if bkg is not None:
                bkg = np.histogram(bkg_data, bins=xedges)[0]
                bkg = np.asarray(bkg, dtype=np.float64)

                # Adjust for measurement duration
                bkg *= bkg_ratio
                bkg_err = np.sqrt(bkg)

                # Subtract background
                spectrum -= bkg
                spectrum[spectrum < 0] = 0
                errors = np.sqrt(errors**2 + bkg_err**2)

            if qeff is not None:
                qeff_val, qeff_err, qeff_x = qeff

                n_qeff = len(qeff_val)

                # Jacobi does not like if spectrum and qeff_val have
                # different shape, so pad qeff_val to the same shape
                if n_qeff < nbins:
                    qeff_val = np.pad(qeff_val, (0, nbins - n_qeff))
                    qeff_err = np.pad(qeff_err, (0, nbins - n_qeff))
                    qeff_x = np.pad(
                        qeff_x,
                        (0, nbins - n_qeff),
                        mode="linear_ramp",
                        end_values=2,
                    )

                xe = xedges / calib[1, 0] - calib[0, 0] / calib[1, 0]
                x = (xe[1:] + xe[:-1]) * 0.5

                def apply_qeff(spec, eff):
                    # Interpolate the efficiencies
                    bin_eff = np.interp(x, qeff_x, eff)

                    # Apply quantum efficiencies
                    nzero = bin_eff > 0
                    spec[nzero] /= bin_eff[nzero]

                    return spec

                spectrum, errors = propagate(
                    apply_qeff,
                    spectrum,
                    errors**2,
                    qeff_val,
                    qeff_err**2,
                    diagonal=True,
                )
                errors = np.sqrt(errors).flatten()

        else:
            errmsg = "uncertainties must be 'montecarlo' or 'gauss'."
            raise ValueError(errmsg)

        # Normalize data to measurement duration per step
        if self._t is not None:
            spectrum /= self._t
            errors /= self._t

        # Normalize data to account for beam intensity, gas
        # pressure, etc.
        """
        for normalize in self._norm:
            val, err = getattr(self, normalize)
            errors = np.sqrt(
                errors**2 / val**2 + err**2 * spectrum**2 / val**4
            )
            spectrum /= val
        """
        # Apply x roi filter
        spectrum[roi[:-1]] = 0
        errors[roi[:-1]] = 0

        # Return the spectrum and uncertainties
        return spectrum, errors

    def transform_norm(self, norm: str, func: callable) -> None:
        """Transform the specified normalization values using a given
        function.

        Parameters
        ----------
        norm: str
            Name of the normalization parameter to transform.
        func: callable
            Function to apply to the normalization values. The function
            should take a single argument of type float and return a
            float.

        """
        val = getattr(self, norm)

        if isinstance(val, np.ndarray):
            val, err = propagate(func, val[0], val[1] ** 2)
            setattr(self, norm, np.array([val, np.sqrt(err)]))

        else:
            setattr(self, norm, func(val))

    def convert_unit(self, norm: str, fro: str, to: str) -> None:
        """Convert the specified normalization values to a different
        unit using the pint package.

        Parameters
        ----------
        norm: str
            Name of the normalization parameter to convert.
        fro: str
            Unit to convert from.
        to: str
            Unit to convert to.

        """
        try:
            from pint import UnitRegistry

        except ImportError as e:
            raise ImportError("pint is required to convert units.") from e

        ureg = UnitRegistry()
        # Convert the normalization values
        self.transform_norm(norm, lambda x: ureg.Quantity(x, fro).m_as(to))


@njit()
def compute_bin(x: float, bin_edges: NDArray) -> int:
    # assuming uniform bins
    n = bin_edges.shape[0] - 1
    a_min = bin_edges[0]
    a_max = bin_edges[-1]

    # special case to mirror NumPy behavior for last bin
    if x == a_max:
        return n - 1  # a_max always in last bin

    bin_idx = int(n * (x - a_min) / (a_max - a_min))

    if bin_idx < 0 or bin_idx >= n:
        return None

    else:
        return bin_idx


@njit()
def numba_histogram(data: NDArray, bin_edges: NDArray) -> NDArray:
    hist = np.zeros((bin_edges.shape[0] - 1,), dtype=np.float64)

    for x in data.flat:
        bin_idx = compute_bin(x, bin_edges)
        if bin_idx is not None:
            hist[int(bin_idx)] += 1

    return hist


@njit()
def numba_weighted_histogram(
    data: NDArray,
    bin_edges: NDArray,
    weights: NDArray,
) -> NDArray:
    hist = np.zeros((bin_edges.shape[0] - 1,), dtype=np.float64)

    for x, w in zip(data.flat, weights.flat):
        bin_idx = compute_bin(x, bin_edges)
        if bin_idx is not None:
            hist[int(bin_idx)] += w

    return hist


@jit(parallel=True, nopython=False)
def montecarlo_spectrum_nobkg(
    spectrum, data, xedges, rng, mc_samples, calib, qeff
):
    # Prepare data
    data_counts = data.shape[0]

    # Prepare the quantum efficiency correction
    qeff_val, qeff_err, qeff_x = qeff
    n_eff = qeff_val.shape[0]

    # Define the interpolation grid
    xe = np.linspace(0, 1, 513)
    x = (xe[1:] + xe[:-1]) * 0.5

    # Assign the data points to the bins
    inds = np.digitize(data[0], xe[1:-1])

    # Start the Monte Carlo simulation
    for i in prange(mc_samples):
        # Create a sample of the efficiencies
        eff_sample = np.ones(n_eff)
        for j in range(n_eff):
            eff_sample[j] = rng.normal(qeff_val[j], qeff_err[j], size=1)[0]

        # Interpolate the efficiencies to get a smoother spectrum
        eff_sample = np.interp(x, qeff_x, eff_sample)

        # Get the efficiencies for each point
        data_eff = 1 / eff_sample[inds]

        # Select data points based on Poisson sampling
        p = rng.poisson(lam=data_counts, size=1)[0]
        poisson_inds = rng.integers(0, data_counts, size=p)
        data_sample = data[poisson_inds]
        data_eff = data_eff[poisson_inds]

        # Convert x values to wavelengths
        a0_sample = rng.normal(calib[0, 0], calib[0, 1], size=1)[0]
        a1_sample = rng.normal(calib[1, 0], calib[1, 1], size=1)[0]
        data_sample = a1_sample * data_sample + a0_sample

        # Calculate the sum of weights for each bin, i.e. the weighted spectrum
        spectrum[i] = numba_weighted_histogram(data_sample, xedges, data_eff)

    # Return the n generated Monte Carlo spectra
    return spectrum


@njit(parallel=True)
def montecarlo_spectrum(
    spectrum, data, xedges, rng, mc_samples, calib, bkg, qeff
):
    # Prepare data
    data_counts = data.shape[0]

    # Prepare the quantum efficiency correction
    qeff_val, qeff_err, qeff_x = qeff
    n_eff = qeff_val.shape[0]

    # Define the interpolation grid
    xe = np.linspace(0, 1, 513)
    x = (xe[1:] + xe[:-1]) * 0.5

    # Assign the data points to the bins
    inds = np.digitize(data[0], xe[1:-1])

    # Prepare the background subtraction
    bkg_data, bkg_ratio = bkg
    bkg_sample_size = int(bkg_data.shape[0] * bkg_ratio)

    # Calculate the background distribution
    bkg_pdf = numba_histogram(bkg_data, xe)

    # Assign the background probabilities to the data points
    bkg_prob = bkg_pdf[inds]

    # Start the Monte Carlo simulation
    for i in prange(mc_samples):
        # Create a sample of the efficiencies
        eff_sample = np.ones(n_eff, dtype=np.float64)
        for j in range(n_eff):
            eff_sample[j] = rng.normal(qeff_val[j], qeff_err[j], size=1)[0]

        # Interpolate the efficiencies to get a smoother spectrum
        eff_sample = np.interp(x, qeff_x, eff_sample)

        # Get the efficiencies for each point
        data_eff = 1 / eff_sample[inds]

        # Select data points based on Poisson sampling
        p = rng.poisson(lam=data_counts, size=1)[0]
        poisson_inds = rng.integers(0, data_counts, size=p)
        data_sample = data[poisson_inds]
        data_eff = data_eff[poisson_inds]
        bkg_sample = bkg_prob[poisson_inds]

        # Remove data points based on the background distribution
        p = rng.poisson(lam=bkg_sample_size, size=1)[0]
        bkg_cdf = np.cumsum(bkg_sample)
        remove_inds = np.searchsorted(bkg_cdf, rng.random(p) * bkg_cdf[-1])
        data_sample = np.delete(data_sample, remove_inds)
        data_eff = np.delete(data_eff, remove_inds)

        # Convert x values to wavelengths
        a0_sample = rng.normal(calib[0, 0], calib[0, 1], size=1)[0]
        a1_sample = rng.normal(calib[1, 0], calib[1, 1], size=1)[0]
        data_sample = a1_sample * data_sample + a0_sample

        # Calculate the sum of weights for each bin, i.e. the weighted spectrum
        spectrum[i] = numba_weighted_histogram(data_sample, xedges, data_eff)

    # Return the n generated Monte Carlo spectra
    return spectrum
